{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ddf93bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "books = pd.read_csv(\n",
    "    'BX-Books.csv',\n",
    "    sep=';',\n",
    "    encoding='latin-1',\n",
    "    engine='python',\n",
    "    on_bad_lines='skip'\n",
    ")\n",
    "\n",
    "users = pd.read_csv(\n",
    "    'BX-Users.csv',\n",
    "    sep=';',\n",
    "    encoding='latin-1',\n",
    "    engine='python',\n",
    "    on_bad_lines='skip'\n",
    ")\n",
    "\n",
    "ratings = pd.read_csv(\n",
    "    'BX-Book-Ratings.csv',\n",
    "    sep=';',\n",
    "    encoding='latin-1',\n",
    "    engine='python',\n",
    "    on_bad_lines='skip'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3fda1cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "books.drop(columns=['Image-URL-S' , 'Image-URL-M'] , inplace=True)\n",
    "\n",
    "books.rename(columns={'Book-Title':'title',\n",
    "                      'Book-Author':'author',\n",
    "                      'Year-Of-Publication':'year',\n",
    "                      'Publisher':'publisher',\n",
    "                      'Image-URL-L':'image-url',\n",
    "                      'ISBN':'isbn'} , inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d0c5f04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "users.rename(columns={'User-ID':'user_id','Location':'location','Age':'age'} , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "10ff15c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.rename(columns={'User-ID':'user_id','ISBN':'isbn','Book-Rating':'rating'} , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "666c4eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>276725</td>\n",
       "      <td>034545104X</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>276726</td>\n",
       "      <td>0155061224</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>276727</td>\n",
       "      <td>0446520802</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>276729</td>\n",
       "      <td>052165615X</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>276729</td>\n",
       "      <td>0521795028</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id        isbn  rating\n",
       "0   276725  034545104X       0\n",
       "1   276726  0155061224       5\n",
       "2   276727  0446520802       0\n",
       "3   276729  052165615X       3\n",
       "4   276729  0521795028       6"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c5bdee45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>276726</td>\n",
       "      <td>0155061224</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>276729</td>\n",
       "      <td>052165615X</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>276729</td>\n",
       "      <td>0521795028</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>276736</td>\n",
       "      <td>3257224281</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>276737</td>\n",
       "      <td>0600570967</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id        isbn  rating\n",
       "1   276726  0155061224       5\n",
       "3   276729  052165615X       3\n",
       "4   276729  0521795028       6\n",
       "6   276736  3257224281       8\n",
       "7   276737  0600570967       6"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = ratings[ratings['rating'] > 0]\n",
    "ratings.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "63c09449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141081, 3)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter users\n",
    "valid_users = ratings['user_id'].value_counts()\n",
    "valid_users = valid_users[valid_users >= 5].index\n",
    "ratings = ratings[ratings['user_id'].isin(valid_users)]\n",
    "\n",
    "# Filter books\n",
    "valid_items = ratings['isbn'].value_counts()\n",
    "valid_items = valid_items[valid_items >= 5].index\n",
    "ratings = ratings[ratings['isbn'].isin(valid_items)]\n",
    "\n",
    "ratings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "06f62a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13030, 11234)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "\n",
    "ratings['user'] = user_encoder.fit_transform(ratings['user_id'])\n",
    "ratings['item'] = item_encoder.fit_transform(ratings['isbn'])\n",
    "\n",
    "num_users = ratings['user'].nunique()\n",
    "num_items = ratings['item'].nunique()\n",
    "\n",
    "num_users, num_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "237c0092",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.sort_values(['user', 'rating'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c84c707a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128051, 5), (13030, 5))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last interaction → test\n",
    "test_df = ratings.groupby('user').tail(1)\n",
    "\n",
    "# rest → train\n",
    "train_df = ratings.drop(test_df.index)\n",
    "\n",
    "train_df.shape, test_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cac29eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "user_tensor = torch.LongTensor(train_df['user'].values)\n",
    "item_tensor = torch.LongTensor(train_df['item'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "848b7da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "train_interactions = defaultdict(set)\n",
    "\n",
    "for u, i in zip(train_df['user'], train_df['item']):\n",
    "    train_interactions[u].add(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 256102 stored elements and shape (24264, 24264)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "num_users = ratings['user'].nunique()\n",
    "num_items = ratings['item'].nunique()\n",
    "\n",
    "# total nodes\n",
    "num_nodes = num_users + num_items\n",
    "\n",
    "# shift item IDs so they don't overlap with users\n",
    "item_ids_shifted = train_df['item'].values + num_users\n",
    "user_ids = train_df['user'].values\n",
    "\n",
    "# for bipartite graph: user ↔ item\n",
    "rows = np.concatenate([user_ids, item_ids_shifted])\n",
    "cols = np.concatenate([item_ids_shifted, user_ids])\n",
    "\n",
    "data = np.ones(len(rows))\n",
    "\n",
    "adj_matrix = csr_matrix((data, (rows, cols)), shape=(num_nodes, num_nodes))\n",
    "\n",
    "adj_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "767d717c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3796/1620851645.py:12: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv_sqrt = np.power(rowsum, -0.5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 256102 stored elements and shape (24264, 24264)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def normalize_adj_matrix(adj):\n",
    "    # convert to COO format\n",
    "    adj = adj.tocoo()\n",
    "    \n",
    "    # compute degree of each node\n",
    "    rowsum = np.array(adj.sum(axis=1)).flatten()\n",
    "\n",
    "    # d^{-1/2}\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5)\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "\n",
    "    # diagonal matrix\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "\n",
    "    # normalized adjacency\n",
    "    return d_mat_inv_sqrt @ adj @ d_mat_inv_sqrt\n",
    "\n",
    "norm_adj_matrix = normalize_adj_matrix(adj_matrix)\n",
    "norm_adj_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1da1dcd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[    1,     1,     1,  ..., 24262, 24263, 24263],\n",
       "                       [14517, 14525, 14842,  ...,  3531,  6320, 12906]]),\n",
       "       values=tensor([0.1667, 0.1508, 0.2041,  ..., 0.4082, 0.1000, 0.3162]),\n",
       "       size=(24264, 24264), nnz=256102, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_sp_mat_to_sp_tensor(sp_mat):\n",
    "    sp_mat = sp_mat.tocoo()\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sp_mat.row, sp_mat.col)).astype(np.int64)\n",
    "    )\n",
    "    values = torch.from_numpy(sp_mat.data.astype(np.float32))\n",
    "    shape = torch.Size(sp_mat.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "norm_adj_tensor = convert_sp_mat_to_sp_tensor(norm_adj_matrix)\n",
    "norm_adj_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6a698ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim, num_layers, norm_adj_matrix):\n",
    "        super().__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # user + item embeddings\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "\n",
    "        # initialize embeddings\n",
    "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.item_embedding.weight)\n",
    "\n",
    "        # store adjacency\n",
    "        self.norm_adj_matrix = norm_adj_matrix.coalesce()\n",
    "\n",
    "    def propagate(self):\n",
    "        \"\"\"Perform LightGCN propagation\"\"\"\n",
    "        all_embeddings = []\n",
    "        \n",
    "        # initial embeddings (layer 0)\n",
    "        user_emb = self.user_embedding.weight\n",
    "        item_emb = self.item_embedding.weight\n",
    "        \n",
    "        emb = torch.cat([user_emb, item_emb])\n",
    "        all_embeddings.append(emb)\n",
    "        \n",
    "        # message passing\n",
    "        for layer in range(self.num_layers):\n",
    "            emb = torch.sparse.mm(self.norm_adj_matrix, emb)\n",
    "            all_embeddings.append(emb)\n",
    "        \n",
    "        # mean of all layers\n",
    "        final_emb = torch.stack(all_embeddings, dim=1).mean(dim=1)\n",
    "        \n",
    "        # split back\n",
    "        final_user_emb = final_emb[:self.num_users]\n",
    "        final_item_emb = final_emb[self.num_users:]\n",
    "        \n",
    "        return final_user_emb, final_item_emb\n",
    "\n",
    "    def forward(self, users, pos_items, neg_items):\n",
    "        user_emb, item_emb = self.propagate()\n",
    "\n",
    "        u = user_emb[users]\n",
    "        pos = item_emb[pos_items]\n",
    "        neg = item_emb[neg_items]\n",
    "        \n",
    "        # BPR scoring\n",
    "        pos_score = torch.sum(u * pos, dim=1)\n",
    "        neg_score = torch.sum(u * neg, dim=1)\n",
    "\n",
    "        return pos_score, neg_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3789398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpr_loss(pos_scores, neg_scores, lambda_reg, user_emb, pos_emb, neg_emb):\n",
    "    mf_loss = -torch.mean(torch.log(torch.sigmoid(pos_scores - neg_scores)))\n",
    "    \n",
    "    reg_loss = lambda_reg * (\n",
    "        user_emb.pow(2).mean() +\n",
    "        pos_emb.pow(2).mean() +\n",
    "        neg_emb.pow(2).mean()\n",
    "    )\n",
    "    \n",
    "    return mf_loss + reg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "670128f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def negative_sampling(user_ids, train_interactions, num_items):\n",
    "    neg_items = []\n",
    "    for u in user_ids:\n",
    "        while True:\n",
    "            neg = np.random.randint(0, num_items)\n",
    "            if neg not in train_interactions[u]:\n",
    "                neg_items.append(neg)\n",
    "                break\n",
    "    return torch.LongTensor(neg_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e96c35d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def train_lightgcn(model, train_df, train_interactions, epochs=15, batch_size=2048, lambda_reg=1e-4, lr=0.001):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    users = torch.LongTensor(train_df['user'].values)\n",
    "    items = torch.LongTensor(train_df['item'].values)\n",
    "\n",
    "    dataset = TensorDataset(users, items)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_users, batch_pos_items in loader:\n",
    "            \n",
    "            # sample negative items\n",
    "            batch_neg_items = negative_sampling(batch_users.numpy(), train_interactions, model.num_items)\n",
    "\n",
    "            # forward pass\n",
    "            pos_scores, neg_scores = model(batch_users, batch_pos_items, batch_neg_items)\n",
    "\n",
    "            # get final embeddings for loss regularization\n",
    "            final_user_emb, final_item_emb = model.propagate()\n",
    "            u_emb = final_user_emb[batch_users]\n",
    "            pos_emb = final_item_emb[batch_pos_items]\n",
    "            neg_emb = final_item_emb[batch_neg_items]\n",
    "\n",
    "            # loss\n",
    "            loss = bpr_loss(pos_scores, neg_scores, lambda_reg, u_emb, pos_emb, neg_emb)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss = {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "22417110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/17 | Loss = 43.4322\n",
      "Epoch 2/17 | Loss = 40.2789\n",
      "Epoch 3/17 | Loss = 35.8292\n",
      "Epoch 4/17 | Loss = 33.0690\n",
      "Epoch 5/17 | Loss = 31.3936\n",
      "Epoch 6/17 | Loss = 30.1939\n",
      "Epoch 7/17 | Loss = 29.0846\n",
      "Epoch 8/17 | Loss = 28.3388\n",
      "Epoch 9/17 | Loss = 27.3250\n",
      "Epoch 10/17 | Loss = 26.7936\n",
      "Epoch 11/17 | Loss = 25.9306\n",
      "Epoch 12/17 | Loss = 25.3387\n",
      "Epoch 13/17 | Loss = 24.6165\n",
      "Epoch 14/17 | Loss = 24.0829\n",
      "Epoch 15/17 | Loss = 23.4760\n",
      "Epoch 16/17 | Loss = 22.8542\n",
      "Epoch 17/17 | Loss = 22.4615\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 64\n",
    "num_layers = 3\n",
    "\n",
    "model = LightGCN(\n",
    "    num_users=num_users,\n",
    "    num_items=num_items,\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_layers=num_layers,\n",
    "    norm_adj_matrix=norm_adj_tensor\n",
    ")\n",
    "\n",
    "train_lightgcn(model, train_df, train_interactions, epochs=17)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b613f66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "test_interactions = defaultdict(set)\n",
    "for u, i in zip(test_df['user'], test_df['item']):\n",
    "    test_interactions[u].add(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f1e5c65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_emb, item_emb = model.propagate()\n",
    "user_emb = user_emb.detach()\n",
    "item_emb = item_emb.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2120dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def recall_at_k(model, user_emb, item_emb, test_interactions, train_interactions, k=10):\n",
    "\n",
    "    recalls = []\n",
    "\n",
    "    all_item_ids = torch.arange(item_emb.shape[0])\n",
    "\n",
    "    for user in test_interactions:\n",
    "        true_items = test_interactions[user]\n",
    "\n",
    "        if len(true_items) == 0:\n",
    "            continue\n",
    "\n",
    "        # items this user interacted with in training\n",
    "        train_items = train_interactions[user]\n",
    "\n",
    "        # score all items\n",
    "        scores = torch.matmul(user_emb[user], item_emb.T)\n",
    "\n",
    "        # mask training interactions (don't recommend items user already read)\n",
    "        scores[list(train_items)] = -1e9\n",
    "\n",
    "        # top-k predicted items\n",
    "        top_k_items = torch.topk(scores, k=k).indices.numpy()\n",
    "\n",
    "        # count how many correct hits\n",
    "        hits = sum([1 for item in true_items if item in top_k_items])\n",
    "\n",
    "        recall = hits / len(true_items)\n",
    "        recalls.append(recall)\n",
    "\n",
    "    return np.mean(recalls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "16510bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@10: 0.022563315425940138\n",
      "Recall@20: 0.03960092095165004\n"
     ]
    }
   ],
   "source": [
    "recall10 = recall_at_k(model, user_emb, item_emb, test_interactions, train_interactions, k=10)\n",
    "recall20 = recall_at_k(model, user_emb, item_emb, test_interactions, train_interactions, k=20)\n",
    "\n",
    "print(\"Recall@10:\", recall10)\n",
    "print(\"Recall@20:\", recall20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baec7a05",
   "metadata": {},
   "source": [
    "### Improving the performance\n",
    "Convert ratings → implicit (1/0)\n",
    "Filter sparse users/items\n",
    "Increase embedding size\n",
    "Train for 100 epochs\n",
    "Tune hyperparameters\n",
    "Re-evaluate Recall@K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a8e54a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dcd3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "61258715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive interactions: 111435\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# df_ratings must have: user_id, isbn, rating\n",
    "implicit_df = ratings.copy()\n",
    "\n",
    "# convert to implicit feedback\n",
    "implicit_df['rating'] = (implicit_df['rating'] >= 7).astype(int)\n",
    "\n",
    "# keep only positive interactions\n",
    "implicit_df = implicit_df[implicit_df['rating'] == 1]\n",
    "\n",
    "print(\"Positive interactions:\", len(implicit_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4fa9ed20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering: (81785, 5)\n"
     ]
    }
   ],
   "source": [
    "# filter users\n",
    "user_counts = implicit_df['user_id'].value_counts()\n",
    "implicit_df = implicit_df[implicit_df['user_id'].isin(user_counts[user_counts >= 5].index)]\n",
    "\n",
    "# filter items\n",
    "item_counts = implicit_df['isbn'].value_counts()\n",
    "implicit_df = implicit_df[implicit_df['isbn'].isin(item_counts[item_counts >= 5].index)]\n",
    "\n",
    "print(\"After filtering:\", implicit_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7954d7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users: 5720 Items: 6821\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "\n",
    "implicit_df[\"user_id_enc\"] = user_encoder.fit_transform(implicit_df[\"user_id\"])\n",
    "implicit_df[\"isbn_enc\"] = item_encoder.fit_transform(implicit_df[\"isbn\"])\n",
    "\n",
    "num_users = implicit_df[\"user_id_enc\"].nunique()\n",
    "num_items = implicit_df[\"isbn_enc\"].nunique()\n",
    "\n",
    "print(\"Users:\", num_users, \"Items:\", num_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d3eb451b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5720, 6821)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# create new IDs\n",
    "user_encoder = {u: i for i, u in enumerate(implicit_df['user_id'].unique())}\n",
    "item_encoder = {i: j for j, i in enumerate(implicit_df['isbn'].unique())}\n",
    "\n",
    "implicit_df['user_id_enc'] = implicit_df['user_id'].map(user_encoder)\n",
    "implicit_df['isbn_enc'] = implicit_df['isbn'].map(item_encoder)\n",
    "\n",
    "num_users = len(user_encoder)\n",
    "num_items = len(item_encoder)\n",
    "\n",
    "num_users, num_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "09aa43bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = implicit_df[[\"user_id_enc\", \"isbn_enc\"]].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2dd82673",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_interactions = (\n",
    "    implicit_df.groupby(\"user_id_enc\")[\"isbn_enc\"]\n",
    "    .apply(set)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# ensure all users exist\n",
    "for u in range(num_users):\n",
    "    if u not in train_interactions:\n",
    "        train_interactions[u] = set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3a68757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def train_lightgcn(model, train_df, train_interactions, epochs=15, batch_size=2048, lambda_reg=1e-4, lr=0.001):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # FIXED COLUMN NAMES\n",
    "    users = torch.LongTensor(train_df['user_id_enc'].values)\n",
    "    items = torch.LongTensor(train_df['isbn_enc'].values)\n",
    "\n",
    "    dataset = TensorDataset(users, items)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_users, batch_pos_items in loader:\n",
    "            \n",
    "            # sample negative items\n",
    "            batch_neg_items = negative_sampling(\n",
    "                batch_users.numpy(),\n",
    "                train_interactions,\n",
    "                model.num_items\n",
    "            )\n",
    "\n",
    "            # forward pass\n",
    "            pos_scores, neg_scores = model(batch_users, batch_pos_items, batch_neg_items)\n",
    "\n",
    "            # final embeddings\n",
    "            final_user_emb, final_item_emb = model.propagate()\n",
    "            u_emb = final_user_emb[batch_users]\n",
    "            pos_emb = final_item_emb[batch_pos_items]\n",
    "            neg_emb = final_item_emb[batch_neg_items]\n",
    "\n",
    "            # BPR loss\n",
    "            loss = bpr_loss(pos_scores, neg_scores, lambda_reg, u_emb, pos_emb, neg_emb)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss = {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c6919b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 81785 stored elements and shape (5720, 6821)>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "rows = implicit_df['user_id_enc'].values\n",
    "cols = implicit_df['isbn_enc'].values\n",
    "\n",
    "interaction_matrix = sp.coo_matrix(\n",
    "    (np.ones(len(implicit_df)), (rows, cols)),\n",
    "    shape=(num_users, num_items)\n",
    ").tocsr()\n",
    "\n",
    "interaction_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c270f99e",
   "metadata": {},
   "source": [
    "## Retrainig the model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5b018698",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_interactions = (\n",
    "    implicit_df.groupby(\"user_id_enc\")[\"isbn_enc\"]\n",
    "    .apply(set)\n",
    "    .to_dict()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5ee3589e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5719, 5720)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(train_interactions.keys()), num_users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2ab13eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_users: 5720\n",
      "num_items: 6821\n",
      "total nodes: 12541\n"
     ]
    }
   ],
   "source": [
    "num_users = train_df['user_id_enc'].max() + 1\n",
    "num_items = train_df['isbn_enc'].max() + 1\n",
    "\n",
    "print(\"num_users:\", num_users)\n",
    "print(\"num_items:\", num_items)\n",
    "print(\"total nodes:\", num_users + num_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "040000b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def build_adj_matrix(train_df, num_users, num_items):\n",
    "    rows = train_df['user_id_enc'].values\n",
    "    cols = train_df['isbn_enc'].values + num_users  # item index shifted\n",
    "\n",
    "    data = np.ones(len(train_df))\n",
    "\n",
    "    # bipartite graph edges (user → item)\n",
    "    adjacency = sp.coo_matrix(\n",
    "        (data, (rows, cols)),\n",
    "        shape=(num_users + num_items, num_users + num_items)\n",
    "    )\n",
    "\n",
    "    # symmetric edges (item → user)\n",
    "    adjacency = adjacency + adjacency.T  \n",
    "\n",
    "    return adjacency.tocsr()\n",
    "\n",
    "adj_matrix = build_adj_matrix(train_df, num_users, num_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e734779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj(adj):\n",
    "    rowsum = np.array(adj.sum(axis=1)).flatten()\n",
    "    d_inv = np.power(rowsum, -0.5)\n",
    "    d_inv[np.isinf(d_inv)] = 0.0\n",
    "    d_mat = sp.diags(d_inv)\n",
    "    return d_mat @ adj @ d_mat\n",
    "\n",
    "norm_adj = normalize_adj(adj_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e0ea2902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sparse_matrix_to_torch_sparse(matrix):\n",
    "    matrix = matrix.tocoo()\n",
    "    indices = torch.from_numpy(np.vstack((matrix.row, matrix.col)).astype(np.int64))\n",
    "    values  = torch.from_numpy(matrix.data.astype(np.float32))\n",
    "    shape   = torch.Size(matrix.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "norm_adj_tensor = convert_sparse_matrix_to_torch_sparse(norm_adj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "89030af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Loss = 13.8582\n",
      "Epoch 2/100 | Loss = 13.8252\n",
      "Epoch 3/100 | Loss = 13.6714\n",
      "Epoch 4/100 | Loss = 13.3033\n",
      "Epoch 5/100 | Loss = 12.7412\n",
      "Epoch 6/100 | Loss = 12.1060\n",
      "Epoch 7/100 | Loss = 11.4923\n",
      "Epoch 8/100 | Loss = 10.9762\n",
      "Epoch 9/100 | Loss = 10.5137\n",
      "Epoch 10/100 | Loss = 10.1415\n",
      "Epoch 11/100 | Loss = 9.8226\n",
      "Epoch 12/100 | Loss = 9.5566\n",
      "Epoch 13/100 | Loss = 9.3453\n",
      "Epoch 14/100 | Loss = 9.1548\n",
      "Epoch 15/100 | Loss = 8.9541\n",
      "Epoch 16/100 | Loss = 8.8072\n",
      "Epoch 17/100 | Loss = 8.6630\n",
      "Epoch 18/100 | Loss = 8.4986\n",
      "Epoch 19/100 | Loss = 8.3884\n",
      "Epoch 20/100 | Loss = 8.2739\n",
      "Epoch 21/100 | Loss = 8.1765\n",
      "Epoch 22/100 | Loss = 8.0467\n",
      "Epoch 23/100 | Loss = 7.8985\n",
      "Epoch 24/100 | Loss = 7.9130\n",
      "Epoch 25/100 | Loss = 7.7637\n",
      "Epoch 26/100 | Loss = 7.6491\n",
      "Epoch 27/100 | Loss = 7.5762\n",
      "Epoch 28/100 | Loss = 7.4621\n",
      "Epoch 29/100 | Loss = 7.4115\n",
      "Epoch 30/100 | Loss = 7.3290\n",
      "Epoch 31/100 | Loss = 7.1471\n",
      "Epoch 32/100 | Loss = 7.1407\n",
      "Epoch 33/100 | Loss = 7.0558\n",
      "Epoch 34/100 | Loss = 6.9135\n",
      "Epoch 35/100 | Loss = 6.8307\n",
      "Epoch 36/100 | Loss = 6.8071\n",
      "Epoch 37/100 | Loss = 6.6916\n",
      "Epoch 38/100 | Loss = 6.6260\n",
      "Epoch 39/100 | Loss = 6.5596\n",
      "Epoch 40/100 | Loss = 6.4417\n",
      "Epoch 41/100 | Loss = 6.4311\n",
      "Epoch 42/100 | Loss = 6.3362\n",
      "Epoch 43/100 | Loss = 6.2486\n",
      "Epoch 44/100 | Loss = 6.1893\n",
      "Epoch 45/100 | Loss = 6.1312\n",
      "Epoch 46/100 | Loss = 6.1000\n",
      "Epoch 47/100 | Loss = 6.0183\n",
      "Epoch 48/100 | Loss = 5.9934\n",
      "Epoch 49/100 | Loss = 5.9205\n",
      "Epoch 50/100 | Loss = 5.7657\n",
      "Epoch 51/100 | Loss = 5.7501\n",
      "Epoch 52/100 | Loss = 5.7232\n",
      "Epoch 53/100 | Loss = 5.6425\n",
      "Epoch 54/100 | Loss = 5.5577\n",
      "Epoch 55/100 | Loss = 5.4859\n",
      "Epoch 56/100 | Loss = 5.4932\n",
      "Epoch 57/100 | Loss = 5.4106\n",
      "Epoch 58/100 | Loss = 5.3952\n",
      "Epoch 59/100 | Loss = 5.3213\n",
      "Epoch 60/100 | Loss = 5.2960\n",
      "Epoch 61/100 | Loss = 5.2231\n",
      "Epoch 62/100 | Loss = 5.1478\n",
      "Epoch 63/100 | Loss = 5.0892\n",
      "Epoch 64/100 | Loss = 5.0791\n",
      "Epoch 65/100 | Loss = 5.0081\n",
      "Epoch 66/100 | Loss = 5.0379\n",
      "Epoch 67/100 | Loss = 4.9267\n",
      "Epoch 68/100 | Loss = 4.8737\n",
      "Epoch 69/100 | Loss = 4.8596\n",
      "Epoch 70/100 | Loss = 4.8855\n",
      "Epoch 71/100 | Loss = 4.7767\n",
      "Epoch 72/100 | Loss = 4.7386\n",
      "Epoch 73/100 | Loss = 4.7005\n",
      "Epoch 74/100 | Loss = 4.6816\n",
      "Epoch 75/100 | Loss = 4.6492\n",
      "Epoch 76/100 | Loss = 4.5932\n",
      "Epoch 77/100 | Loss = 4.5461\n",
      "Epoch 78/100 | Loss = 4.4983\n",
      "Epoch 79/100 | Loss = 4.4684\n",
      "Epoch 80/100 | Loss = 4.4429\n",
      "Epoch 81/100 | Loss = 4.3791\n",
      "Epoch 82/100 | Loss = 4.3546\n",
      "Epoch 83/100 | Loss = 4.3240\n",
      "Epoch 84/100 | Loss = 4.3250\n",
      "Epoch 85/100 | Loss = 4.2444\n",
      "Epoch 86/100 | Loss = 4.2533\n",
      "Epoch 87/100 | Loss = 4.1374\n",
      "Epoch 88/100 | Loss = 4.1111\n",
      "Epoch 89/100 | Loss = 4.0398\n",
      "Epoch 90/100 | Loss = 4.0500\n",
      "Epoch 91/100 | Loss = 3.9738\n",
      "Epoch 92/100 | Loss = 3.9961\n",
      "Epoch 93/100 | Loss = 3.9801\n",
      "Epoch 94/100 | Loss = 3.9649\n",
      "Epoch 95/100 | Loss = 3.9312\n",
      "Epoch 96/100 | Loss = 3.8274\n",
      "Epoch 97/100 | Loss = 3.8209\n",
      "Epoch 98/100 | Loss = 3.7895\n",
      "Epoch 99/100 | Loss = 3.7989\n",
      "Epoch 100/100 | Loss = 3.7604\n"
     ]
    }
   ],
   "source": [
    "model = LightGCN(\n",
    "    num_users=num_users,\n",
    "    num_items=num_items,\n",
    "    embedding_dim=64,\n",
    "    num_layers=3,\n",
    "    norm_adj_matrix=norm_adj_tensor\n",
    ")\n",
    "\n",
    "train_lightgcn(\n",
    "    model=model,\n",
    "    train_df=train_df,\n",
    "    train_interactions=train_interactions,\n",
    "    batch_size=4096,\n",
    "    epochs=100,\n",
    "    lambda_reg=1e-4,\n",
    "    lr=0.001\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2a1f8439",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'user_id_enc'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[105]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ground truth interactions from test set (encoded IDs)\u001b[39;00m\n\u001b[32m      2\u001b[39m test_interactions = (\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43mtest_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser_id_enc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33misbn_enc\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      4\u001b[39m            .apply(\u001b[38;5;28mset\u001b[39m)\n\u001b[32m      5\u001b[39m            .to_dict()\n\u001b[32m      6\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/frame.py:9210\u001b[39m, in \u001b[36mDataFrame.groupby\u001b[39m\u001b[34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m   9207\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   9208\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou have to supply one of \u001b[39m\u001b[33m'\u001b[39m\u001b[33mby\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlevel\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m9210\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   9211\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   9212\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9213\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9214\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9216\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9217\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9218\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9220\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1331\u001b[39m, in \u001b[36mGroupBy.__init__\u001b[39m\u001b[34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m   1328\u001b[39m \u001b[38;5;28mself\u001b[39m.dropna = dropna\n\u001b[32m   1330\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1331\u001b[39m     grouper, exclusions, obj = \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1336\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1338\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1339\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib.no_default:\n\u001b[32m   1342\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping._passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper.groupings):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/groupby/grouper.py:1043\u001b[39m, in \u001b[36mget_grouper\u001b[39m\u001b[34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[39m\n\u001b[32m   1041\u001b[39m         in_axis, level, gpr = \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1042\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[32m   1044\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr.key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1045\u001b[39m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[32m   1046\u001b[39m     exclusions.add(gpr.key)\n",
      "\u001b[31mKeyError\u001b[39m: 'user_id_enc'"
     ]
    }
   ],
   "source": [
    "# ground truth interactions from test set (encoded IDs)\n",
    "test_interactions = (\n",
    "    test_df.groupby(\"user_id_enc\")[\"isbn_enc\"]\n",
    "           .apply(set)\n",
    "           .to_dict()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e418e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
